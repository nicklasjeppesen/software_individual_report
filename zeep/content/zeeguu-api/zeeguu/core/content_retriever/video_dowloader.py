import csv
import os
import zeeguu
import zeeguu.core.model
from zeeguu.core.constants import SIMPLE_TIME_FORMAT

from zeeguu.core.elastic.indexing import index_in_elasticsearch
from zeeguu.core.model import Url, Article, Language
import datetime

db_session = zeeguu.core.model.db.session

TITLE = 0
DESCRIPTION = 1
URL = 2
CHANNEL = 3
PUBLISHED_AT = 4
PUBLISH_TIME = 5
QUERY = 6
CAPTIONS = 7
AUTOGENERATED = 8
TRANSLATED = 9
SRC_LANG = 10


# videoId,title,description,url,channelId,publishedAt,publishTime,query,captions,autogenerated,translated,src_lang


def get_videos_from_local_csv(filename):
    __location__ = os.path.realpath(
        os.path.join(os.getcwd(), os.path.dirname(__file__))
    )

    with open(os.path.join(__location__, filename), "r", encoding="utf8") as f:
        print(f)
        reader = csv.reader(f)

        result = {}

        for row in reader:
            key = row[0]
            if key in result:
                # implement your duplicate row handling here
                pass
            result[key] = row[1:]

        return result


def download(filename, fr, to):
    ai_videos = get_videos_from_local_csv(filename)

    keys = list(ai_videos.keys())
    for each in keys[fr:to]:
        video_info = ai_videos[each]

        # stupid file format
        video_info[TITLE] = video_info[TITLE].replace("&#39;", "'")

        e = download_individual_video(video_info)


def download_individual_video(video_info):
    print("Processing: " + video_info[URL])

    from zeeguu.core.model import Article

    new_article = Article.find(video_info[URL])
    if new_article:
        print("FOUND ARTICLE ALREADY")
        # session.delete(found)
        # session.commit()

    dt = datetime.datetime.strptime(video_info[PUBLISH_TIME], SIMPLE_TIME_FORMAT)
    # dt = datetime.datetime.now()

    fr = Language.find("fr")

    if new_article == None:
        print("not found... creating it")

        url_object = Url.find_or_create(db_session, video_info[URL])

        print(video_info[PUBLISH_TIME])

        new_article = Article(
            url_object,
            video_info[TITLE],
            "",
            video_info[CAPTIONS],  # any article longer than this will be truncated...
            video_info[DESCRIPTION],
            dt,
            None,
            fr,
            "",
            video=1,
        )
    else:
        print(f"UPDATING ARTICLE ... {new_article.id}")
        new_article.title = video_info[TITLE]
        print(new_article.title)
        new_article.summary = video_info[DESCRIPTION]
        new_article.content = video_info[CAPTIONS]
        new_article.publish_time = dt

    db_session.add(new_article)
    db_session.commit()

    index_in_elasticsearch(new_article, db_session)


import sys

if __name__ == "__main__":

    print("run as main...")
    if len(sys.argv) > 3:
        print("sufficiennt args...")
        download(sys.argv[1], int(sys.argv[2]), int(sys.argv[3]))
    else:
        download("artificial_intelligence.csv", 1, 50)
